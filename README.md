# ğŸ§  Personal Offline AI Assistant

A **local, offline, privacy-first AI assistant** built using **Ollama + Mistral + Python**.
The assistant can read personal documents (PDFs, notes), answer questions using them (RAG),
maintain long-term memory, and help with daily study planning â€” all without internet access.

---

## âœ¨ Features

- ğŸ”’ **Fully Offline & Private** (no cloud APIs)
- ğŸ“š **Document-Aware (RAG)** â€“ answers from your PDFs & notes
- ğŸ§  **Persistent Memory** (with user consent)
- ğŸ“… **Daily Planner Mode**
- ğŸ–¥ï¸ **Desktop UI (Tkinter)**
- âš¡ **Optimized for CPU-only systems**

---

## ğŸ—ï¸ Architecture Overview

- **Ollama + Mistral** â†’ Local LLM inference  
- **Python** â†’ Orchestration & logic  
- **Keyword-based Retrieval (RAG)** â†’ Lightweight & fast  
- **Tkinter** â†’ Desktop UI  
- **JSON Memory** â†’ Safe, explicit long-term memory  

---

## ğŸ“‚ Project Structure

personal_ai_assistant/
â”œâ”€â”€ assistant.py # Core logic (RAG, memory, planner)
â”œâ”€â”€ ui.py # Desktop UI
â”œâ”€â”€ memory.json # Persistent memory store
â”œâ”€â”€ ai_docs/ # User documents (PDF/TXT)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸš€ How to Run

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt

#start ollama and pull model
ollama pull mistral

#run the assistant UI
python ui.py



ğŸ§ª Example Commands

plan my day

Explain OSI model

Remember that I am studying Computer Networks



ğŸ§  Learning Outcomes

Built a full RAG pipeline from scratch

Designed safe memory management

Optimized LLM usage for low-resource systems

Implemented threaded UI for responsiveness

Learned practical LLM system design



ğŸ“Œ Future Improvements

Semantic embeddings for retrieval

PDF OCR support

Web or mobile UI

Model switching (Mistral / LLaMA)


ğŸ‘¤ Author

Abhay
Computer Engineering Student
Interested in AI Systems, Data Engineering, and LLMs

